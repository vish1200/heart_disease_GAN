{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import time\n",
    "from IPython import display\n",
    "import math\n",
    "import time\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "# import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minMaxNormalise(data, cont_ls):\n",
    "    for j in cont_ls:\n",
    "        index = list(data.index.values)\n",
    "        df_ = pd.DataFrame(index=index, columns=[j])\n",
    "        for i in index:\n",
    "            df_.at[i,j] = (data.at[i,j]-data[j].min())/(data[j].max()-data[j].min())\n",
    "        data.at[:,j] = np.nan\n",
    "        for k in index:\n",
    "            data.at[k,j] = df_.at[k,j]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanNormalise(data, cont_ls):\n",
    "    for j in cont_ls:\n",
    "        index = list(data.index.values)\n",
    "        df_ = pd.DataFrame(index=index, columns=[j])\n",
    "        for i in index:\n",
    "            df_.at[i,j] = (data.at[i,j]-data[j].mean())/(data[j].std())\n",
    "        data.at[:,j] = np.nan\n",
    "        for k in index:\n",
    "            data.at[k,j] = df_.at[k,j]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEnc(data, cat_ls):\n",
    "    for i in cat_ls:\n",
    "        col_df = data[i]\n",
    "        encoded_df = pd.get_dummies(col_df, prefix = [i])\n",
    "        data = data.drop([i], axis=1)\n",
    "        data = pd.concat([data, encoded_df], axis = 1, sort=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z(m, n):\n",
    "    seed = np.random.uniform(0., 1., size=[m, n])\n",
    "    seed = np.sort(seed)\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242, 11)\n",
      "(61,)\n",
      "80.33\n",
      "(303, 14)\n",
      "count    303.000000\n",
      "mean       0.544554\n",
      "std        0.498835\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        1.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "Name: target, dtype: float64\n",
      "(242, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('heart-disease-uci.zip')\n",
    "data_process = False\n",
    "if data_process:    \n",
    "    cont_ls = ['age', 'trestbps', 'chol', 'thalach','oldpeak']\n",
    "    data = minMaxNormalise(data, cont_ls)\n",
    "    cat_col = ['cp', 'slope', 'thal']\n",
    "    data = oneHotEnc(data, cat_col)\n",
    "\n",
    "target_df = data['target']\n",
    "variables = data.drop(['target'], axis=1)\n",
    "variables = variables.drop(['cp', 'slope', 'thal'], axis=1)\n",
    "\n",
    "data_train, data_test, label_train, label_test = train_test_split(variables, target_df, test_size=0.20)\n",
    "gan_data = pd.concat([data_train, label_train], axis=1, sort=False)\n",
    "print(gan_data.shape)\n",
    "print(label_test.shape)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(data_train, label_train)\n",
    "accuracy = round(logreg.score(data_test, label_test)*100, 2)\n",
    "print(accuracy)\n",
    "print(data.shape)\n",
    "print(target_df.describe())\n",
    "print(data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 13)\n",
      "(303,)\n",
      "[[-3.01661313e-01  6.62090003e-01 -4.11581755e-01  4.74634409e-01\n",
      "  -3.38076726e-02  2.45603681e-01  2.32952029e-01 -9.65954661e-02\n",
      "   2.20368698e-01 -2.61301696e-01  3.10562700e-01 -3.38573784e-01\n",
      "   3.44081759e-01]\n",
      " [ 1.11989349e-01 -1.43439978e-01  3.11427400e-03 -2.85573512e-01\n",
      "   2.85463240e-02 -1.55123249e-01  3.14072054e-03 -7.56887794e-02\n",
      "  -1.24019630e-01 -6.63599193e-01  2.18883544e-01  5.65915763e-01\n",
      "  -7.27179348e-01]\n",
      " [-1.22202910e-01 -6.93527758e-02  9.93300974e-02  1.26315832e-01\n",
      "  -1.51919678e-01 -2.95511223e-02 -2.33785156e-02 -3.25395525e-01\n",
      "   1.99739501e-01  8.46362948e-01 -8.44541013e-01 -1.02210507e-01\n",
      "   4.14442718e-01]\n",
      " [ 2.89429694e-01  3.25679839e-01 -5.95579267e-01 -2.58272231e-01\n",
      "  -4.79031533e-01 -4.18461055e-01  2.97389776e-01 -6.54131696e-02\n",
      "  -1.76671147e-01  3.53872955e-01 -2.26733580e-01  1.62143663e-01\n",
      "  -1.80221677e-01]\n",
      " [ 1.92813892e-02  1.01137406e-03 -3.89686853e-01  1.93303004e-02\n",
      "   4.16743150e-03  6.51056826e-01 -3.45443130e-01 -4.06570226e-01\n",
      "  -1.43538803e-01  3.58200043e-01 -2.20169649e-01  5.16259730e-01\n",
      "  -5.73452413e-01]\n",
      " [ 3.78091127e-01  4.37696613e-02 -2.74044275e-01 -2.65772313e-01\n",
      "   2.10105389e-01 -4.21738416e-01  4.14401561e-01  1.27956597e-02\n",
      "   4.52633590e-01  1.19983159e-01 -4.63936448e-01 -2.87780046e-01\n",
      "   2.12154031e-01]\n",
      " [ 7.34934330e-01 -6.56922519e-01 -1.07993460e+00 -1.07414335e-01\n",
      "  -1.71055138e-01 -3.59230161e-01  1.85497567e-01 -3.20102185e-01\n",
      "  -1.56356618e-01 -4.77712452e-01 -2.70215273e-01  4.12494987e-01\n",
      "  -1.51394114e-01]\n",
      " [-3.40715528e-01  4.19951789e-02 -3.96968842e-01 -5.63768148e-01\n",
      "  -2.45902255e-01  1.08956106e-01 -4.36161816e-01 -1.97198808e-01\n",
      "  -5.58965266e-01 -4.23597060e-02  4.38161522e-01  3.81149873e-02\n",
      "   2.09829375e-01]\n",
      " [ 6.28450334e-01 -4.25087333e-01 -2.45633677e-01 -4.68163304e-02\n",
      "   2.09342644e-01 -1.88500434e-01 -2.76394308e-01 -2.96778888e-01\n",
      "   8.52329358e-02 -6.31733775e-01  7.27453887e-01  2.64096111e-02\n",
      "   4.70911376e-02]\n",
      " [-7.17353642e-01 -4.86124456e-02  6.18581355e-01 -2.45531783e-01\n",
      "   3.32610190e-01 -2.22057208e-01  2.11091697e-01 -9.49538946e-02\n",
      "  -3.77697229e-01 -2.96800938e-02  3.24343145e-01  4.94962752e-01\n",
      "  -5.24877965e-01]\n",
      " [-7.01418698e-01 -7.98317716e-02  1.23417139e-01 -2.31406167e-02\n",
      "   1.48810357e-01 -2.34474421e-01 -1.61586747e-01 -2.18166709e-01\n",
      "  -2.93958277e-01 -1.73783768e-02 -2.71839082e-01 -4.26184028e-01\n",
      "   3.70477140e-01]\n",
      " [ 3.27148773e-02 -6.90638602e-01  8.44668865e-01 -5.57948053e-01\n",
      "  -3.97414446e-01 -2.98127383e-01  3.25159192e-01 -1.12875633e-01\n",
      "  -3.64762366e-01  6.56475902e-01 -4.55555677e-01  1.20886274e-01\n",
      "   1.20292135e-01]\n",
      " [ 4.03575212e-01  2.30410337e-01 -5.47821343e-01 -2.54129708e-01\n",
      "  -5.21878779e-01  9.66865122e-02  2.13487625e-01 -3.85480583e-01\n",
      "   2.57329583e-01 -1.29958197e-01  3.00499946e-01  5.09215057e-01\n",
      "  -6.10768557e-01]\n",
      " [ 2.37212613e-01  6.30466565e-02 -1.93639874e-01 -9.98863354e-02\n",
      "  -2.18948379e-01 -1.76964715e-01  2.05777332e-01  7.74370804e-02\n",
      "  -2.19933376e-01 -1.03162095e-01  1.76643535e-01  3.26613009e-01\n",
      "   1.88007131e-01]\n",
      " [-6.62787676e-01 -6.74191058e-01  5.97219229e-01  2.63301075e-01\n",
      "   1.33065894e-01 -7.05641866e-01  4.92775232e-01  5.07799610e-02\n",
      "   9.99675393e-02 -2.72766203e-01  5.21896303e-01  7.25335777e-02\n",
      "   2.42062077e-01]\n",
      " [-5.33075213e-01  1.69583395e-01  2.25431696e-01  2.68568307e-01\n",
      "   1.42327502e-01  2.00436026e-01 -3.68342906e-01  1.91876248e-01\n",
      "  -3.81949753e-01 -2.35860363e-01  1.06852226e-01  3.47496450e-01\n",
      "  -5.84097981e-01]\n",
      " [-3.42374109e-02 -4.77230042e-01  8.15620273e-02 -1.62497595e-01\n",
      "  -6.77812755e-01  6.12141907e-01 -1.67546608e-02 -1.96670383e-01\n",
      "  -3.93157750e-01 -3.98804128e-01 -4.43393923e-02 -2.32353900e-03\n",
      "  -8.71469527e-02]\n",
      " [ 5.29312864e-02 -2.61446297e-01 -1.82775427e-02 -5.46854198e-01\n",
      "  -2.02656940e-01 -2.24809751e-01  6.47099316e-01 -4.88696396e-01\n",
      "  -4.73092422e-02  4.70626168e-03  3.35737556e-01 -6.97404146e-01\n",
      "   3.90404224e-01]\n",
      " [-4.85633701e-01 -2.58149296e-01  2.20871881e-01  2.00269699e-01\n",
      "  -2.22207874e-01  6.12721264e-01 -8.45412612e-01 -4.48847562e-01\n",
      "  -1.18038235e-02  4.15635109e-01 -5.38140774e-01  2.02397467e-03\n",
      "   4.73128676e-01]\n",
      " [ 1.50391413e-02  2.90209234e-01  2.04426065e-01 -7.05988467e-01\n",
      "  -2.78093278e-01 -1.04221344e+00  9.88238454e-01 -2.55871326e-01\n",
      "  -4.56481576e-01  3.97497684e-01 -4.44090396e-01 -2.74520993e-01\n",
      "   3.08846235e-01]\n",
      " [ 1.95681289e-01  3.85533683e-02  1.94079340e-01 -5.89895368e-01\n",
      "   3.28411132e-01  2.98088580e-01 -5.27597368e-01 -3.65110755e-01\n",
      "   1.67145148e-01 -3.36021930e-01  4.84264880e-01 -6.80025667e-02\n",
      "  -1.02902785e-01]\n",
      " [-8.48339915e-01  4.54948395e-01  1.29898652e-01  1.15648434e-01\n",
      "  -3.73603314e-01 -3.34142633e-02  3.15614849e-01 -2.47539192e-01\n",
      "  -1.03461379e-02 -5.71876049e-01  3.24936688e-01 -1.02859236e-01\n",
      "   2.57077157e-01]\n",
      " [-2.45095119e-01  2.61088818e-01 -1.28232598e-01  1.22595742e-01\n",
      "   1.04737498e-01 -3.42612237e-01  1.40633404e-01 -2.16913167e-02\n",
      "   1.54957235e-01  1.21878758e-01 -1.93752698e-04 -1.97378352e-01\n",
      "  -1.22628529e-02]\n",
      " [-1.00371502e-01 -6.72039211e-01  6.12865150e-01 -2.61583894e-01\n",
      "  -5.63017309e-01  9.87254083e-01 -9.25666332e-01 -4.30421233e-01\n",
      "  -7.43692741e-02 -4.98820513e-01 -1.78054214e-01 -4.24236506e-01\n",
      "   5.31183541e-01]\n",
      " [ 7.66079187e-01 -5.30293882e-01 -3.89079332e-01 -1.65302306e-01\n",
      "  -4.21658546e-01  1.68968707e-01 -2.94201553e-01 -3.92967969e-01\n",
      "   3.03680867e-01 -5.78050278e-02 -5.57127774e-01  6.28892958e-01\n",
      "  -4.52171952e-01]\n",
      " [-2.33004257e-01  3.56832057e-01 -5.84208071e-01  8.14836845e-02\n",
      "  -2.92435557e-01 -4.35795963e-01  5.81737638e-01  2.04867907e-02\n",
      "  -2.89834589e-01  9.06092599e-02 -7.72137716e-02 -4.82144237e-01\n",
      "   5.22246063e-01]\n",
      " [ 1.45471081e-01 -1.61087751e-01  3.18548381e-01  8.21479857e-02\n",
      "  -2.49036014e-01  1.45641267e-01 -6.06193841e-02  1.30748421e-01\n",
      "   1.08458005e-01 -1.04551882e-01 -5.96546158e-02  1.51976839e-01\n",
      "   7.69406259e-02]\n",
      " [ 3.85420918e-01 -2.97205389e-01 -8.52405280e-02  5.71551695e-02\n",
      "   3.89794894e-02  7.47476876e-01 -5.68028271e-01 -7.85613507e-02\n",
      "  -1.77646726e-01  5.02232790e-01  1.33873761e-01 -5.31245172e-02\n",
      "  -2.64860868e-01]\n",
      " [-3.27946246e-02  3.33867639e-01 -3.99723411e-01  5.14651015e-02\n",
      "  -1.90677345e-01  1.33533716e-01 -1.70869231e-01  1.32426605e-01\n",
      "  -5.81843033e-02 -5.27602136e-02  6.29670322e-02 -4.12189752e-01\n",
      "   2.24132417e-03]\n",
      " [ 2.10782617e-01 -8.27853009e-02 -2.03276306e-01 -2.89091110e-01\n",
      "  -3.70420069e-01  3.83292496e-01 -3.29573721e-01  2.22789332e-01\n",
      "  -1.93881188e-02  1.20674282e-01 -1.07667304e-03  7.89583802e-01\n",
      "  -7.17650950e-01]\n",
      " [ 3.11022431e-01  1.51852205e-01  1.21466905e-01  6.00215644e-02\n",
      "  -3.58680636e-01 -5.35832345e-01  6.47432745e-01 -3.67902368e-01\n",
      "  -5.06106555e-01  3.55011433e-01 -2.57826328e-01 -3.34226638e-01\n",
      "   2.31415704e-01]\n",
      " [ 1.07354239e-01 -1.61710352e-01 -1.69269264e-01 -1.95331335e-01\n",
      "  -1.34824842e-01  2.03437895e-01 -2.85455525e-01  1.15770504e-01\n",
      "  -1.68346837e-01  1.50119960e-02  2.80775148e-02  1.62253588e-01\n",
      "  -2.76439428e-01]\n",
      " [-2.34493345e-01  2.56947696e-01  1.66727051e-01  2.09683576e-03\n",
      "   2.21417859e-01  1.06489010e-01 -2.89514601e-01  2.56912887e-01\n",
      "   2.98896402e-01  4.24511917e-02  7.44885802e-02 -1.19468838e-01\n",
      "  -9.49589685e-02]\n",
      " [ 5.34832403e-02 -8.74544159e-02 -7.09321797e-02 -1.24650702e-01\n",
      "   1.24679595e-01  5.21832407e-01 -3.16498786e-01 -1.29846305e-01\n",
      "  -2.72821695e-01  5.76750755e-01 -4.38576907e-01 -5.47396958e-01\n",
      "   5.78086555e-01]\n",
      " [-2.80382723e-01  3.70094121e-01 -6.02165163e-01  5.18973947e-01\n",
      "   6.05288625e-01 -4.24109995e-01 -3.48509938e-01 -4.72158611e-01\n",
      "  -2.61823535e-01  2.33973354e-01 -1.53238475e-01  2.25056499e-01\n",
      "  -4.18486297e-01]\n",
      " [-5.29589839e-02  3.77182327e-02  1.04427949e-01  5.69621682e-01\n",
      "  -2.91407760e-02 -1.85340181e-01 -1.93537444e-01 -2.15706807e-02\n",
      "   2.03153521e-01  2.81059414e-01  1.34359032e-01 -4.78485040e-02\n",
      "  -1.13387965e-01]\n",
      " [-7.67567754e-03  2.43643194e-01  2.35779077e-01  1.77426785e-01\n",
      "  -3.59662473e-02 -5.97284138e-02 -2.53609717e-01  1.64653093e-01\n",
      "   2.14421481e-01  3.41049135e-02 -2.91031063e-01  2.13172942e-01\n",
      "   3.15549046e-01]\n",
      " [-5.49057126e-01  2.09977835e-01  1.52540222e-01  1.52694419e-01\n",
      "  -1.80929378e-01 -1.27612233e-01 -3.43341827e-01 -2.02915683e-01\n",
      "  -2.69198209e-01  1.91812724e-01 -5.76560199e-01 -5.20896018e-01\n",
      "   9.65138152e-02]\n",
      " [ 4.16089416e-01 -5.83294511e-01 -4.85732704e-01  2.01435015e-01\n",
      "   2.60324091e-01  7.88193196e-02  1.10338004e-02  9.98394638e-02\n",
      "  -6.15211546e-01  4.21738058e-01  2.45983243e-01 -8.57366025e-02\n",
      "  -5.68747260e-02]\n",
      " [ 2.07645372e-01  2.18144078e-02  8.17098245e-02  2.61497438e-01\n",
      "  -8.77659544e-02  4.24137861e-02 -1.00711994e-02  7.56918788e-02\n",
      "  -1.17210764e-02 -1.13276556e-01  2.60744035e-01  2.33658642e-01\n",
      "  -2.49419913e-01]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session\n",
    "tf.reset_default_graph()\n",
    "data = pd.read_csv('heart-disease-uci.zip')\n",
    "cat_col = ['cp', 'slope', 'thal', 'target']\n",
    "data_cat = data[cat_col]\n",
    "data_cat = oneHotEnc(data_cat, cat_col)\n",
    "dummy_labels = np.ones([data_cat.shape[0]])\n",
    "print(data_cat.shape)\n",
    "print(dummy_labels.shape)\n",
    "data_train, data_test, _, __, = train_test_split(data_cat, dummy_labels, test_size = 0.1)\n",
    "\n",
    "encoding_dim = 40\n",
    "input_ = Input(shape=(data_cat.shape[1], ))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_)\n",
    "decoding_layer = Dense(data_cat.shape[1], activation = 'sigmoid')\n",
    "decoded = decoding_layer(encoded)\n",
    "\n",
    "autoencoder = Model(input_, decoded)\n",
    "encoder = Model(input_, encoded)\n",
    "\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "autoencoder.fit(data_train, data_train, epochs=50, validation_data=(data_test, data_test), verbose=False)\n",
    "\n",
    "# print(data_test.iloc[1].values.reshape(data_test.shape[1],1).shape)\n",
    "# print(data_test)\n",
    "autoencoded_data = decoder.predict(encoder.predict(data_test)).round(0)\n",
    "print(autoencoder.get_weights()[-2])\n",
    "print(type(autoencoded_data))\n",
    "print(type(data_test.values))\n",
    "print(np.sum(np.sum((abs(autoencoded_data - data_test.values)), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 13)\n",
      "(303,)\n",
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 0\t Discriminator loss: 0.8587\t Generator loss: 0.6059\n",
      "Iterations: 1000\t Discriminator loss: 0.1627\t Generator loss: 2.1480\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session\n",
    "tf.reset_default_graph()\n",
    "data = pd.read_csv('heart-disease-uci.zip')\n",
    "cat_col = ['cp', 'slope', 'thal', 'target']\n",
    "data_cat = data[cat_col]\n",
    "data_cat = oneHotEnc(data_cat, cat_col)\n",
    "dummy_labels = np.ones([data_cat.shape[0]])\n",
    "print(data_cat.shape)\n",
    "print(dummy_labels.shape)\n",
    "data_train, data_test, _, __, = train_test_split(data_cat, dummy_labels, test_size = 0.1)\n",
    "\n",
    "encoding_dim = 40\n",
    "input_ = Input(shape=(data_cat.shape[1], ))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_)\n",
    "decoding_layer = Dense(data_cat.shape[1], activation = 'sigmoid')\n",
    "decoded = decoding_layer(encoded)\n",
    "\n",
    "autoencoder = Model(input_, decoded)\n",
    "encoder = Model(input_, encoded)\n",
    "\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "autoencoder.fit(data_train, data_train, epochs=50, validation_data=(data_test, data_test), verbose=False)\n",
    "\n",
    "# print(data_test.iloc[1].values.reshape(data_test.shape[1],1).shape)\n",
    "# print(data_test)\n",
    "# decoder.predict(encoder.predict(data_test)).round(0)\n",
    "# print(decoder.predict(encoder.predict(data_test.iloc[1].values.reshape(data_test.shape[1]))))\n",
    "\n",
    "# print(autoencoder.layers[-1].get_weights()[0])\n",
    "##############################################################\n",
    "def my_innit(shape, dtype=None):\n",
    "    a = autoencoder.get_weights()[-2]\n",
    "    return a\n",
    "\n",
    "def my_binnit(shape, dtype=None):\n",
    "    b = autoencoder.get_weights()[-1]\n",
    "    return b\n",
    "\n",
    "num_points = encoding_dim\n",
    "print(num_points)\n",
    "gen_input_size = 30\n",
    "optimizer = Adam(0.0002, 0.9)\n",
    "batch_size = int(data_cat.shape[0]*0.20)\n",
    "epochs = 5001\n",
    "steps_per_epoch = 5\n",
    "def generator():\n",
    "    generator = Sequential()\n",
    "    \n",
    "    generator.add(Dense(100, input_dim=gen_input_size))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    generator.add(Dense(60, input_dim=gen_input_size))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    generator.add(Dense(encoding_dim))\n",
    "#     generator.add(BatchNormalization())\n",
    "#     generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "#     generator.add(Dense(encoding_dim))\n",
    "    generator.add(Dense(data_cat.shape[1], kernel_initializer = my_innit, bias_initializer = my_binnit, trainable=False, activation='sigmoid'))\n",
    "    \n",
    "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return generator\n",
    "\n",
    "def discriminator():\n",
    "    discriminator = Sequential()\n",
    "    \n",
    "    discriminator.add(Dense(100, input_dim=data_cat.shape[1]))\n",
    "    discriminator.add(BatchNormalization())\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    discriminator.add(Dense(50))\n",
    "    discriminator.add(BatchNormalization())\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    discriminator.compile(loss = 'binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return discriminator\n",
    "\n",
    "discriminator = discriminator()\n",
    "generator = generator()\n",
    "\n",
    "discriminator.trainable = False\n",
    "\n",
    "gan_input = Input(shape=(gen_input_size,))\n",
    "generated_patient = generator(gan_input)\n",
    "gan_output = discriminator(generated_patient)\n",
    "\n",
    "gan = Model(gan_input, gan_output)\n",
    "\n",
    "gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    for steps in range(steps_per_epoch):\n",
    "        noise = sample_z(batch_size, gen_input_size)\n",
    "        fake_x = generator.predict(noise)\n",
    "        real_x = data_cat.sample(n=batch_size)\n",
    "        x = np.concatenate((real_x, fake_x))\n",
    "        disc_y = np.zeros(2*batch_size)\n",
    "        disc_y[:batch_size] = 0.9\n",
    "        d_loss = discriminator.train_on_batch(x, disc_y)\n",
    "        y_gen = np.ones(batch_size)\n",
    "        g_loss = gan.train_on_batch(noise, y_gen)\n",
    "        \n",
    "    if epoch%1000 == 0:\n",
    "        print(\"Iterations: %d\\t Discriminator loss: %.4f\\t Generator loss: %.4f\"%(epoch,d_loss,g_loss))\n",
    "print((time.time()-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cp']_0 2.0\n",
      "['cp']_1 47.0\n",
      "['cp']_2 54.0\n",
      "['cp']_3 0.0\n",
      "['slope']_0 1.0\n",
      "['slope']_1 59.0\n",
      "['slope']_2 44.0\n",
      "['thal']_0 0.0\n",
      "['thal']_1 0.0\n",
      "['thal']_2 0.0\n",
      "['thal']_3 101.0\n",
      "['target']_0 99.0\n",
      "['target']_1 0.0\n",
      "#######################################################################\n",
      "['cp']_0 41.94\n",
      "['cp']_1 32.26\n",
      "['cp']_2 25.81\n",
      "['cp']_3 0.0\n",
      "['slope']_0 9.68\n",
      "['slope']_1 38.71\n",
      "['slope']_2 51.61\n",
      "['thal']_0 0.0\n",
      "['thal']_1 6.45\n",
      "['thal']_2 41.94\n",
      "['thal']_3 51.61\n",
      "['target']_0 51.61\n",
      "['target']_1 48.39\n"
     ]
    }
   ],
   "source": [
    "a = generator.predict(sample_z(1, gen_input_size))\n",
    "cohort_size = 100\n",
    "for i in range(cohort_size):\n",
    "    a = np.concatenate((a, generator.predict(sample_z(1, gen_input_size))), axis=0)\n",
    "# print(data_cat.shape[1])\n",
    "data = a\n",
    "data_df = pd.DataFrame(data=data, columns = data_cat.columns.values)\n",
    "# print(data_df)\n",
    "# print(data_test)\n",
    "for i in data_df.columns.values:\n",
    "    print(i, ((np.sum(data_df[i].values.round(0)))))\n",
    "print(\"#######################################################################\")\n",
    "for i in data_df.columns.values:\n",
    "    print(i, ((np.sum(data_test[i].values)/data_test.shape[0])*100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07364900509516398\n",
      "Iterations: 0\t Discriminator loss: 1.0702\t Generator loss: 0.7048\n",
      "0.710802686214447\n",
      "Iterations: 1000\t Discriminator loss: 0.3579\t Generator loss: 2.9466\n",
      "1.3496827999750773\n",
      "Iterations: 2000\t Discriminator loss: 0.3403\t Generator loss: 2.2525\n",
      "1.9891611377398173\n",
      "Iterations: 3000\t Discriminator loss: 0.2877\t Generator loss: 2.5429\n",
      "2.6289148847262065\n",
      "Iterations: 4000\t Discriminator loss: 0.3638\t Generator loss: 2.6033\n",
      "3.268787682056427\n",
      "Iterations: 5000\t Discriminator loss: 1.0531\t Generator loss: 2.4568\n",
      "3.9091056187947593\n",
      "Iterations: 6000\t Discriminator loss: 0.8747\t Generator loss: 0.9968\n",
      "4.563680469989777\n",
      "Iterations: 7000\t Discriminator loss: 0.3759\t Generator loss: 2.0373\n",
      "5.207132951418559\n",
      "Iterations: 8000\t Discriminator loss: 0.2806\t Generator loss: 2.8732\n",
      "5.849877270062764\n",
      "Iterations: 9000\t Discriminator loss: 0.3487\t Generator loss: 1.9284\n",
      "6.50902335246404\n",
      "Iterations: 10000\t Discriminator loss: 0.4468\t Generator loss: 1.8486\n",
      "7.1528817017873125\n",
      "Iterations: 11000\t Discriminator loss: 0.3990\t Generator loss: 2.0635\n",
      "7.797216419378916\n",
      "Iterations: 12000\t Discriminator loss: 0.3261\t Generator loss: 2.7228\n",
      "8.441002082824706\n",
      "Iterations: 13000\t Discriminator loss: 0.1909\t Generator loss: 3.4284\n",
      "9.1194140513738\n",
      "Iterations: 14000\t Discriminator loss: 0.3706\t Generator loss: 1.6511\n",
      "9.772337218125662\n",
      "Iterations: 15000\t Discriminator loss: 0.3947\t Generator loss: 1.2029\n",
      "10.419859365622203\n",
      "Iterations: 16000\t Discriminator loss: 0.3600\t Generator loss: 2.0692\n",
      "11.06068293650945\n",
      "Iterations: 17000\t Discriminator loss: 0.5395\t Generator loss: 2.8460\n",
      "11.705550932884217\n",
      "Iterations: 18000\t Discriminator loss: 0.2980\t Generator loss: 2.1074\n",
      "12.35326810280482\n",
      "Iterations: 19000\t Discriminator loss: 0.4178\t Generator loss: 1.5959\n",
      "13.011628687381744\n",
      "Iterations: 20000\t Discriminator loss: 0.6124\t Generator loss: 3.1686\n",
      "13.65462883313497\n",
      "Iterations: 21000\t Discriminator loss: 0.3099\t Generator loss: 2.8253\n",
      "14.304896501700084\n",
      "Iterations: 22000\t Discriminator loss: 0.3185\t Generator loss: 2.2080\n",
      "14.951633818944295\n",
      "Iterations: 23000\t Discriminator loss: 0.3269\t Generator loss: 2.0620\n",
      "15.592554787794748\n",
      "Iterations: 24000\t Discriminator loss: 0.2120\t Generator loss: 4.7404\n",
      "16.231361031532288\n",
      "Iterations: 25000\t Discriminator loss: 0.2881\t Generator loss: 2.4071\n",
      "16.870867184797923\n",
      "Iterations: 26000\t Discriminator loss: 0.4227\t Generator loss: 2.0083\n",
      "17.515040866533916\n",
      "Iterations: 27000\t Discriminator loss: 0.5034\t Generator loss: 1.7666\n",
      "18.15403453509013\n",
      "Iterations: 28000\t Discriminator loss: 0.5205\t Generator loss: 2.4863\n",
      "18.796862185001373\n",
      "Iterations: 29000\t Discriminator loss: 0.4332\t Generator loss: 2.3182\n",
      "19.43720423380534\n",
      "Iterations: 30000\t Discriminator loss: 0.3208\t Generator loss: 1.8380\n",
      "20.077979453404744\n",
      "Iterations: 31000\t Discriminator loss: 0.2503\t Generator loss: 2.6534\n",
      "20.719825100898742\n",
      "Iterations: 32000\t Discriminator loss: 0.3117\t Generator loss: 2.0845\n",
      "21.358835717042286\n",
      "Iterations: 33000\t Discriminator loss: 0.3704\t Generator loss: 2.0058\n",
      "22.055008471012115\n",
      "Iterations: 34000\t Discriminator loss: 0.2768\t Generator loss: 2.4101\n",
      "22.747820083300272\n",
      "Iterations: 35000\t Discriminator loss: 0.6554\t Generator loss: 2.5886\n",
      "23.421154352029166\n",
      "Iterations: 36000\t Discriminator loss: 0.3653\t Generator loss: 1.6693\n",
      "24.083066900571186\n",
      "Iterations: 37000\t Discriminator loss: 0.4612\t Generator loss: 2.0037\n",
      "24.725662950674693\n",
      "Iterations: 38000\t Discriminator loss: 0.3376\t Generator loss: 1.9405\n",
      "25.374271134535473\n",
      "Iterations: 39000\t Discriminator loss: 0.3022\t Generator loss: 2.1398\n",
      "26.017313583691916\n",
      "Iterations: 40000\t Discriminator loss: 0.4073\t Generator loss: 2.8431\n",
      "26.659748351573946\n",
      "Iterations: 41000\t Discriminator loss: 0.3605\t Generator loss: 2.4362\n",
      "27.302017800013225\n",
      "Iterations: 42000\t Discriminator loss: 0.3782\t Generator loss: 1.7614\n",
      "27.944067533810934\n",
      "Iterations: 43000\t Discriminator loss: 0.4399\t Generator loss: 1.7995\n",
      "28.584266169865927\n",
      "Iterations: 44000\t Discriminator loss: 0.4878\t Generator loss: 2.1415\n",
      "29.23018986781438\n",
      "Iterations: 45000\t Discriminator loss: 0.5282\t Generator loss: 1.2607\n",
      "29.878043417135874\n",
      "Iterations: 46000\t Discriminator loss: 0.4967\t Generator loss: 1.8961\n",
      "30.52568426926931\n",
      "Iterations: 47000\t Discriminator loss: 0.3286\t Generator loss: 2.1783\n",
      "31.235307451089223\n",
      "Iterations: 48000\t Discriminator loss: 0.5277\t Generator loss: 1.4052\n",
      "31.879460187753043\n",
      "Iterations: 49000\t Discriminator loss: 0.2994\t Generator loss: 2.3721\n",
      "32.54600040117899\n",
      "Iterations: 50000\t Discriminator loss: 0.6868\t Generator loss: 1.8595\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('heart-disease-uci.zip')\n",
    "data_process = False\n",
    "if data_process:    \n",
    "    cont_ls = ['age', 'trestbps', 'chol', 'thalach','oldpeak']\n",
    "    data = minMaxNormalise(data, cont_ls)\n",
    "    cat_col = ['cp', 'slope', 'thal']\n",
    "    data = oneHotEnc(data, cat_col)\n",
    "\n",
    "target_df = data['target']\n",
    "variables = data.drop(['target'], axis=1)\n",
    "\n",
    "data_train, data_test, label_train, label_test = train_test_split(variables, target_df, test_size=0.20)\n",
    "gan_data = pd.concat([data_train, label_train], axis=1, sort=False)\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "tf.reset_default_graph()\n",
    "tf.keras.backend.clear_session\n",
    "start = time.time()\n",
    "num_points = gan_data.shape[1]\n",
    "print(num_points)\n",
    "gen_input_size = 100\n",
    "optimizer = Adam(0.0002, 0.9)\n",
    "batch_size = data_train.shape[0]\n",
    "epochs = 50001\n",
    "steps_per_epoch = 1\n",
    "def generator():\n",
    "    generator = Sequential()\n",
    "    \n",
    "    generator.add(Dense(500, input_dim=gen_input_size))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    generator.add(Dense(200))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    generator.add(Dense(50))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    generator.add(Dense(num_points))\n",
    "    \n",
    "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return generator\n",
    "\n",
    "def discriminator():\n",
    "    discriminator = Sequential()\n",
    "    \n",
    "    discriminator.add(Dense(200, input_dim=num_points))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    discriminator.add(Dense(50, input_dim=num_points))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    \n",
    "#     discriminator.add(Dense(50, input_dim=num_points))\n",
    "#     discriminator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    discriminator.compile(loss = 'binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return discriminator\n",
    "\n",
    "discriminator = discriminator()\n",
    "generator = generator()\n",
    "\n",
    "discriminator.trainable = False\n",
    "\n",
    "gan_input = Input(shape=(gen_input_size,))\n",
    "generated_patient = generator(gan_input)\n",
    "gan_output = discriminator(generated_patient)\n",
    "\n",
    "gan = Model(gan_input, gan_output)\n",
    "\n",
    "gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for steps in range(steps_per_epoch):\n",
    "        noise = sample_z(batch_size, gen_input_size)\n",
    "        fake_x = generator.predict(noise)\n",
    "        real_x = gan_data.sample(n=batch_size)\n",
    "        x = np.concatenate((real_x, fake_x))\n",
    "        disc_y = np.zeros(2*batch_size)\n",
    "        disc_y[:batch_size] = 0.9\n",
    "        d_loss = discriminator.train_on_batch(x, disc_y)\n",
    "        y_gen = np.ones(batch_size)\n",
    "        g_loss = gan.train_on_batch(noise, y_gen)\n",
    "        \n",
    "    if epoch%1000 == 0:\n",
    "        print((time.time()-start)/60)\n",
    "        print(\"Iterations: %d\\t Discriminator loss: %.4f\\t Generator loss: %.4f\"%(epoch,d_loss,g_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 14)\n",
      "count    242.000000\n",
      "mean       0.528926\n",
      "std        0.500197\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        1.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "Name: target, dtype: float64\n",
      "0\n",
      "Event rate in generated cohort: 0.00\n",
      "Event rate in test cohort: 0.61\n",
      "61\n",
      "37\n",
      "          age       sex        cp    trestbps        chol       fbs   restecg  \\\n",
      "0   99.684799  2.990803  1.109957  217.372803  339.374847  1.112384  0.166245   \n",
      "1  106.844696  3.159410  1.204707  232.597748  363.650482  1.212611  0.181576   \n",
      "2  116.297806  3.421916  1.315457  252.955475  395.680878  1.336409  0.201762   \n",
      "3  101.011963  3.015105  1.133956  220.234299  343.896088  1.127637  0.173851   \n",
      "4  102.971252  3.062208  1.160759  224.207687  350.319977  1.170142  0.177121   \n",
      "5  110.840881  3.243651  1.269091  240.856262  376.932373  1.275178  0.184759   \n",
      "6  109.073158  3.222993  1.226477  237.557419  371.303375  1.233913  0.186988   \n",
      "7  108.227135  3.157053  1.248366  235.001465  367.670837  1.255879  0.175291   \n",
      "8  110.379707  3.223912  1.264347  240.049988  375.719910  1.253366  0.185258   \n",
      "9  110.972725  3.242553  1.273971  241.287186  377.818451  1.259027  0.182036   \n",
      "\n",
      "      thalach     exang   oldpeak     slope        ca      thal    target  \n",
      "0  217.953049  4.211187 -0.261510  2.379620 -0.360258  2.982415  0.077338  \n",
      "1  232.969070  4.473681 -0.259989  2.489863 -0.370763  3.217036  0.069374  \n",
      "2  253.124496  4.852528 -0.272332  2.686090 -0.398614  3.506588  0.067683  \n",
      "3  220.813477  4.249836 -0.254556  2.392767 -0.358379  3.021451  0.069180  \n",
      "4  224.506332  4.324203 -0.266336  2.406922 -0.356832  3.092449  0.072618  \n",
      "5  240.986801  4.615690 -0.255254  2.537355 -0.362393  3.342765  0.056979  \n",
      "6  237.922409  4.563066 -0.263778  2.543898 -0.376568  3.280521  0.067876  \n",
      "7  234.867874  4.495790 -0.250245  2.442425 -0.347671  3.260172  0.051449  \n",
      "8  240.305542  4.592549 -0.255848  2.532110 -0.366182  3.336514  0.065710  \n",
      "9  241.593262  4.624881 -0.261334  2.548807 -0.368224  3.362910  0.075449  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-b2e1bfb3eb99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mlogreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1549\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1550\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    877\u001b[0m             raise ValueError(\"This solver needs samples of at least 2 classes\"\n\u001b[1;32m    878\u001b[0m                              \u001b[0;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                              \" class: %r\" % classes_[0])\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mclass_weight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0"
     ]
    }
   ],
   "source": [
    "a = generator.predict(sample_z(1, gen_input_size))\n",
    "cohort_size = 1000\n",
    "for i in range(cohort_size):\n",
    "    a = np.concatenate((a, generator.predict(sample_z(1, gen_input_size))), axis=0)\n",
    "print(a.shape)\n",
    "print(gan_data['target'].describe())\n",
    "b = pd.DataFrame(a, columns=gan_data.columns.values.tolist())\n",
    "target = b['target']\n",
    "variables = b.drop(['target'], axis=1)\n",
    "# print(target.head(10))\n",
    "target_ls = []\n",
    "target = target.round(0)\n",
    "for i in target:\n",
    "    if i>=1:\n",
    "        target_ls.append(1)\n",
    "    else:\n",
    "        target_ls.append(0)\n",
    "# print(target.describe())\n",
    "# print(gan_data.sample(n=1))\n",
    "total = 0\n",
    "for i in target_ls:\n",
    "    if i == 1:\n",
    "        total = total + 1\n",
    "        \n",
    "total_l = 0\n",
    "for i in label_test:\n",
    "    if i == 1:\n",
    "        total_l = total_l + 1\n",
    "print(total)\n",
    "\n",
    "print(\"Event rate in generated cohort: %.2f\" %(total/cohort_size))\n",
    "print(\"Event rate in test cohort: %.2f\" %(total_l/len(label_test)))\n",
    "print(len(label_test))\n",
    "print(total_l)\n",
    "print(b.head(10))\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(variables, target_ls)\n",
    "accuracy = round(logreg.score(data_test, label_test)*100, 2)\n",
    "print(accuracy)\n",
    "print(confusion_matrix(label_test, logreg.predict(data_test)))\n",
    "print(label_test.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
