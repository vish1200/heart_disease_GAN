{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import glob\n",
    "# import imageio\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "# import PIL\n",
    "from tensorflow import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, Reshape, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Input\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import time\n",
    "# from IPython import display\n",
    "import math\n",
    "import time\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "# import tensorflow as tf\n",
    "# import json\n",
    "# from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minMaxNormalise(data, cont_ls):\n",
    "    for j in cont_ls:\n",
    "        index = list(data.index.values)\n",
    "        df_ = pd.DataFrame(index=index, columns=[j])\n",
    "        for i in index:\n",
    "            df_.at[i,j] = (data.at[i,j]-data[j].min())/(data[j].max()-data[j].min())\n",
    "        data.at[:,j] = np.nan\n",
    "        for k in index:\n",
    "            data.at[k,j] = df_.at[k,j]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanNormalise(data, cont_ls):\n",
    "    for j in cont_ls:\n",
    "        index = list(data.index.values)\n",
    "        df_ = pd.DataFrame(index=index, columns=[j])\n",
    "        for i in index:\n",
    "            df_.at[i,j] = (data.at[i,j]-data[j].mean())/(data[j].std())\n",
    "        data.at[:,j] = np.nan\n",
    "        for k in index:\n",
    "            data.at[k,j] = df_.at[k,j]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode categorical variable columns\n",
    "def oneHotEnc(data, cat_ls):\n",
    "    for i in cat_ls:\n",
    "        col_df = data[i]\n",
    "        encoded_df = pd.get_dummies(col_df, prefix = [i])\n",
    "        data = data.drop([i], axis=1)\n",
    "        data = pd.concat([data, encoded_df], axis = 1, sort=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate random input for GAN generator\n",
    "def sample_z(m, n):\n",
    "    seed = np.random.uniform(0., 1., size=[m, n])\n",
    "    seed = np.sort(seed)\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contToCat(data, num_bins, columns):\n",
    "    data = data\n",
    "    for i in columns:\n",
    "        bin_sz = math.floor((data[i].max() - data[i].min())/num_bins)\n",
    "        bins = []\n",
    "        for a in range(0, num_bins+1):\n",
    "            bins.append(data[i].min() + a*bin_sz)\n",
    "        data[i] = pd.cut(x=data[i], bins = bins)\n",
    "    data = oneHotEnc(data, columns)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session\n",
    "tf.reset_default_graph()\n",
    "start = time.time()\n",
    "data = pd.read_csv('heart-disease-uci.zip')\n",
    "cat_col = ['cp', 'slope', 'thal', 'target'] #columns in dataframe which contain categorical variables\n",
    "# data_cat = data[cat_col] #Select columns in dataframe which contain categorical variables\n",
    "data_cat = oneHotEnc(data, cat_col) \n",
    "num_bins = 5\n",
    "cont_ls = ['age', 'trestbps', 'chol', 'thalach','oldpeak']\n",
    "data_cat_2 = contToCat(data_cat, num_bins, cont_ls)\n",
    "cohort_size = 50 #size of the cohort which we want GAN to simulate (i.e. 50 patients)\n",
    "data_train, data_test, _, _ = train_test_split(data_cat_2, data_cat_2, test_size=0.33) #split dataset into train and test, test will only be used to test accuracy of GAN generated patients\n",
    "data_train_list = []\n",
    "data_test_list = []\n",
    "for i in range(300): #Here we are creating 300 training and test samples of 50 patients each \n",
    "    data_train_list.append(data_train.sample(cohort_size).values.flatten())        \n",
    "    data_test_list.append(data_test.sample(cohort_size).values.flatten())\n",
    "\n",
    "#Reshape train and test lists into arrays of 300 x cohort size*number of features per patient\n",
    "data_train_cohort = np.reshape(data_train_list, (300, cohort_size*data_cat_2.shape[1]))\n",
    "data_test_cohort = np.reshape(data_test_list, (300, cohort_size*data_cat_2.shape[1]))\n",
    "\n",
    "data_train_cohort = pd.DataFrame(data_train_cohort)\n",
    "print(data_cat_2.shape[1])\n",
    "#Autoencoder code\n",
    "num_filters = 200\n",
    "kernel_size = 100\n",
    "strides = 5\n",
    "encoding_dim = 50000\n",
    "encoded_input_sz = (((cohort_size*data_cat_2.shape[1] - kernel_size)/strides)+1)*num_filters\n",
    "input_ = Input(shape=(cohort_size*data_cat_2.shape[1],), name = \"Input\")\n",
    "input__ = Reshape((cohort_size*data_cat_2.shape[1],1))(input_)\n",
    "# conv_1D = Conv1D(filters = num_filters, kernel_size = kernel_size, strides = strides, input_shape = (cohort_size*data_cat_2.shape[1],1), name = \"conv_layer\")\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_)\n",
    "# encoded = conv_1D(input__)\n",
    "# encoded_1 = Flatten()(encoded) #Only for conv1D\n",
    "decoding_layer = Dense(cohort_size*data_cat_2.shape[1], activation = 'sigmoid', name = \"layer_4\")\n",
    "# decoded = decoding_layer(encoded_1) #Only for Conv1D\n",
    "decoded = decoding_layer(encoded)\n",
    "\n",
    "autoencoder = Model(input_, decoded)\n",
    "encoder = Model(input_, encoded)\n",
    "\n",
    "# encoded_input = Input(shape=(encoded_input_sz,))\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "decoder_layer_3 = autoencoder.layers[-1](encoded_input)\n",
    "decoder = Model(encoded_input, decoder_layer_3)\n",
    "\n",
    "autoencoder.compile(optimizer='Adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(data_train_cohort, data_train_cohort, epochs=500, validation_data=(data_test_cohort, data_test_cohort), verbose=False)\n",
    "print((time.time()-start)/60)\n",
    "# print(data_test.iloc[1].values.reshape(data_test.shape[1],1).shape)\n",
    "# print(data_test)\n",
    "# decoder.predict(encoder.predict(data_test)).round(0)\n",
    "# print(decoder.predict(encoder.predict(data_test.iloc[1].values.reshape(data_test.shape[1]))))\n",
    "\n",
    "# print(autoencoder.layers[-1].get_weights()[0])\n",
    "##############################################################\n",
    "autoencoded_data = decoder.predict(encoder.predict(data_test_cohort)).round(0)\n",
    "accuracy_list = []\n",
    "_0 = 0\n",
    "_1 = 0\n",
    "_2 = 0\n",
    "_3 = 0\n",
    "for i in range(autoencoded_data.shape[0]):\n",
    "    total = 0\n",
    "    for a in range(cohort_size):\n",
    "        predicted = np.reshape(autoencoded_data[i], (cohort_size, data_cat_2.shape[1]))[a]\n",
    "        test = np.reshape(data_test_cohort[i], (cohort_size, data_cat_2.shape[1]))[a]\n",
    "        x = np.sum(abs(predicted - test))\n",
    "        if x == 0:\n",
    "            total = total + 1\n",
    "            _0 = _0 + 1\n",
    "        if x == 1:\n",
    "            _1 = _1 + 1\n",
    "        if x == 2:\n",
    "            _2 = _2 + 1\n",
    "        if x >= 3:\n",
    "            _3 = _3 + 1\n",
    "    accuracy_list.append(total/cohort_size)\n",
    "print(np.mean(accuracy_list))\n",
    "print(autoencoded_data.shape)\n",
    "print(data_test.shape)\n",
    "total = 0\n",
    "for i in range(data_test_cohort.shape[0]):\n",
    "    for a in range(data_test_cohort.shape[1]):\n",
    "        if data_test_cohort[i,a] == autoencoded_data[i,a]:\n",
    "            total = total+1\n",
    "print(\"Accuracy % per variable: \", total/(data_test_cohort.shape[0]*data_test.shape[1]))\n",
    "print(\"0 errors\", _0/(cohort_size*data_test_cohort.shape[0]))\n",
    "print(\"1 error\", _1/(cohort_size*data_test_cohort.shape[0]))\n",
    "print(\"2 errors\", _2/(cohort_size*data_test_cohort.shape[0]))\n",
    "print(\"3 errors\", _3/(cohort_size*data_test_cohort.shape[0]))\n",
    "\n",
    "# Can I save the autoencoder and reload it later so I don't have to train it everytime? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-83bc97425fe9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mgan_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_input_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mgenerated_patient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgan_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0mgan_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_patient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 463\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 463\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    893\u001b[0m                                       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                                       \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                                       constraint=self.kernel_constraint)\n\u001b[0m\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             self.bias = self.add_weight(shape=(self.units,),\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         weight = K.variable(initializer(shape, dtype=dtype),\n\u001b[0m\u001b[1;32m    280\u001b[0m                             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-83bc97425fe9>\u001b[0m in \u001b[0;36mmy_innit\u001b[0;34m(shape, dtype)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmy_innit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnew_autoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mnew_autoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_autoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session\n",
    "tf.reset_default_graph()\n",
    "def my_innit(shape, dtype=None): #Get weights from decoding layer of autoencoder\n",
    "    new_autoencoder = keras.Model.from_config(config)\n",
    "    new_autoencoder.set_weights(weights)\n",
    "    a = new_autoencoder.get_weights()[-2]\n",
    "    return a\n",
    "\n",
    "def my_binnit(shape, dtype=None): #Get biases from decoding layer of autoencoder\n",
    "    new_autoencoder = keras.Model.from_config(config)\n",
    "    new_autoencoder.set_weights(weights)\n",
    "    b = new_autoencoder.get_weights()[-1]\n",
    "    return b\n",
    "\n",
    "# num_points = encoding_dim\n",
    "# print(num_points)\n",
    "gen_input_size = 15000 #Input size to generator\n",
    "optimizer = Adam(0.0002, 0.9)\n",
    "batch_size = 100\n",
    "epochs = 500\n",
    "steps_per_epoch = 1\n",
    "def generator():\n",
    "    generator = Sequential()\n",
    "        \n",
    "#     generator.add(Dense(5000, input_dim=gen_input_size))\n",
    "#     generator.add(Activation('relu'))\n",
    "#     generator.add(BatchNormalization())\n",
    "    \n",
    "    generator.add(Dense(11100))\n",
    "#     generator.add(Activation('relu'))\n",
    "#     generator.add(Dense(10000))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(Activation('relu'))\n",
    "    \n",
    "#     generator.add(Dense(10000, input_dim=gen_input_size))\n",
    "#     generator.add(BatchNormalization())\n",
    "#     generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "#     generator.add(Dense(7100))\n",
    "#     generator.add(BatchNormalization())\n",
    "#     generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "#     generator.add(Dense(encoding_dim))\n",
    "    generator.add(Dense(data_train_cohort.shape[1], kernel_initializer = my_innit, bias_initializer = my_binnit, trainable=False, activation='sigmoid'))\n",
    "    \n",
    "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return generator\n",
    "\n",
    "def discriminator():\n",
    "    discriminator = Sequential()\n",
    "    \n",
    "    discriminator.add(Dense(11100, input_dim=data_train_cohort.shape[1]))\n",
    "    discriminator.add(BatchNormalization())\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    \n",
    "#     discriminator.add(Dense(50))\n",
    "#     discriminator.add(BatchNormalization())\n",
    "#     discriminator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    discriminator.compile(loss = 'binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return discriminator\n",
    "\n",
    "discriminator = discriminator()\n",
    "generator = generator()\n",
    "\n",
    "discriminator.trainable = False\n",
    "\n",
    "gan_input = Input(shape=(gen_input_size,))\n",
    "generated_patient = generator(gan_input)\n",
    "gan_output = discriminator(generated_patient)\n",
    "\n",
    "gan = Model(gan_input, gan_output)\n",
    "\n",
    "gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    for steps in range(steps_per_epoch):\n",
    "        noise = sample_z(batch_size, gen_input_size)\n",
    "        fake_x = generator.predict(noise)\n",
    "        real_x = data_train_cohort.sample(n=batch_size)\n",
    "        x = np.concatenate((real_x, fake_x))\n",
    "        disc_y = np.zeros(2*batch_size)\n",
    "        disc_y[:batch_size] = 0.9\n",
    "        d_loss = discriminator.train_on_batch(x, disc_y)\n",
    "        y_gen = np.ones(batch_size)\n",
    "        g_loss = gan.train_on_batch(noise, y_gen)\n",
    "        \n",
    "    if epoch%1000 == 0:\n",
    "        print(\"Iterations: %d\\t Discriminator loss: %.4f\\t Generator loss: %.4f\"%(epoch,d_loss,g_loss))\n",
    "        print((time.time()-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2deb700ca4fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_input_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnum_patients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_patients\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcohort_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_input_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generator' is not defined"
     ]
    }
   ],
   "source": [
    "a = generator.predict(sample_z(1, gen_input_size)) #Generate cohort of patients of size cohort size from generator\n",
    "num_patients = 10000 #Number of patients we want to generate - note this should be divisible by cohort_size\n",
    "for i in range((int(num_patients/cohort_size))):\n",
    "    a = np.concatenate((a, generator.predict(sample_z(1, gen_input_size))), axis=0)\n",
    "print(a.shape)\n",
    "fake_data = np.reshape(a, (int(a.shape[1]/data_cat.shape[1]*a.shape[0]), data_cat.shape[1])) #Reshape generated patients array so you have 1 patient per row\n",
    "fake_data = pd.DataFrame(b, columns=data_cat.columns.values)\n",
    "target_ls = []\n",
    "target_df = fake_data[data_cat.columns.values[11]] #This is our target variable from the fake dataset - whether the patient was hospitalised\n",
    "for i in range(fake_data.shape[0]):\n",
    "    if target_df[i] ==1:\n",
    "        target_ls.append(1)\n",
    "    else:\n",
    "        target_ls.append(0)\n",
    "print(sum(target_ls))        \n",
    "fake_data = fake_data.drop([data_cat.columns.values[11], data_cat.columns.values[12]], axis=1) #Remove the target variable to give us our fake training dataset\n",
    "test_label = data_test[data_cat.columns.values[11]] #Creating the test label from our test dataset which we created at the start\n",
    "test_data = data_test.drop([data_cat.columns.values[11], data_cat.columns.values[12]], axis=1) #creating test dataset\n",
    "real_train_data = data_train #Training dataset made up of our real data \n",
    "real_train_label = real_train_data[data_cat.columns.values[11]] #Labet data from real dataset\n",
    "real_train_data = real_train_data.drop([data_cat.columns.values[11], data_cat.columns.values[12]], axis=1)\n",
    "# variable_train, test_data, label_train, test_label = train_test_split(test_data, test_label, test_size=0.20)\n",
    "\n",
    "#Develop logistic regression models using real and fake data - Objective is to build \n",
    "# a model using fake data which is at least equal to the model using real data\n",
    "logreg_gan = LogisticRegression()\n",
    "logreg_real = LogisticRegression()\n",
    "logreg_gan.fit(b, target_ls)\n",
    "logreg_real.fit(real_train_data, real_train_label)\n",
    "accuracy_gan = round(logreg_gan.score(test_data, test_label)*100, 2)\n",
    "accuracy_real = round(logreg_real.score(test_data, test_label)*100, 2)\n",
    "\n",
    "print(accuracy_gan)\n",
    "print(accuracy_real)\n",
    "print(confusion_matrix(test_label, logreg_gan.predict(test_data)))\n",
    "# print(label_test.describe())\n",
    "# print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
