{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import time\n",
    "from IPython import display\n",
    "import math\n",
    "import time\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "# import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minMaxNormalise(data, cont_ls):\n",
    "    for j in cont_ls:\n",
    "        index = list(data.index.values)\n",
    "        df_ = pd.DataFrame(index=index, columns=[j])\n",
    "        for i in index:\n",
    "            df_.at[i,j] = (data.at[i,j]-data[j].min())/(data[j].max()-data[j].min())\n",
    "        data.at[:,j] = np.nan\n",
    "        for k in index:\n",
    "            data.at[k,j] = df_.at[k,j]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanNormalise(data, cont_ls):\n",
    "    for j in cont_ls:\n",
    "        index = list(data.index.values)\n",
    "        df_ = pd.DataFrame(index=index, columns=[j])\n",
    "        for i in index:\n",
    "            df_.at[i,j] = (data.at[i,j]-data[j].mean())/(data[j].std())\n",
    "        data.at[:,j] = np.nan\n",
    "        for k in index:\n",
    "            data.at[k,j] = df_.at[k,j]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEnc(data, cat_ls):\n",
    "    for i in cat_ls:\n",
    "        col_df = data[i]\n",
    "        encoded_df = pd.get_dummies(col_df, prefix = [i])\n",
    "        data = data.drop([i], axis=1)\n",
    "        data = pd.concat([data, encoded_df], axis = 1, sort=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z(m, n):\n",
    "    seed = np.random.uniform(0., 1., size=[m, n])\n",
    "    seed = np.sort(seed)\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242, 11)\n",
      "(61,)\n",
      "80.33\n",
      "(303, 14)\n",
      "count    303.000000\n",
      "mean       0.544554\n",
      "std        0.498835\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        1.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "Name: target, dtype: float64\n",
      "(242, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('heart-disease-uci.zip')\n",
    "data_process = False\n",
    "if data_process:    \n",
    "    cont_ls = ['age', 'trestbps', 'chol', 'thalach','oldpeak']\n",
    "    data = minMaxNormalise(data, cont_ls)\n",
    "    cat_col = ['cp', 'slope', 'thal']\n",
    "    data = oneHotEnc(data, cat_col)\n",
    "\n",
    "target_df = data['target']\n",
    "variables = data.drop(['target'], axis=1)\n",
    "variables = variables.drop(['cp', 'slope', 'thal'], axis=1)\n",
    "\n",
    "data_train, data_test, label_train, label_test = train_test_split(variables, target_df, test_size=0.20)\n",
    "gan_data = pd.concat([data_train, label_train], axis=1, sort=False)\n",
    "print(gan_data.shape)\n",
    "print(label_test.shape)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(data_train, label_train)\n",
    "accuracy = round(logreg.score(data_test, label_test)*100, 2)\n",
    "print(accuracy)\n",
    "print(data.shape)\n",
    "print(target_df.describe())\n",
    "print(data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 13)\n",
      "(303,)\n",
      "30\n",
      "5751.576303764185\n",
      "Iterations: 0\t Discriminator loss: 1.0263\t Generator loss: 0.5622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5752.37605048418\n",
      "Iterations: 1000\t Discriminator loss: 0.1707\t Generator loss: 1.0466\n",
      "5753.162052170435\n",
      "Iterations: 2000\t Discriminator loss: 0.1645\t Generator loss: 1.3631\n",
      "5753.947297934691\n",
      "Iterations: 3000\t Discriminator loss: 0.1634\t Generator loss: 1.5173\n",
      "5754.737648121516\n",
      "Iterations: 4000\t Discriminator loss: 0.1631\t Generator loss: 1.6746\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session\n",
    "tf.reset_default_graph()\n",
    "data = pd.read_csv('heart-disease-uci.zip')\n",
    "cat_col = ['cp', 'slope', 'thal', 'target']\n",
    "data_cat = data[cat_col]\n",
    "data_cat = oneHotEnc(data_cat, cat_col)\n",
    "dummy_labels = np.ones([data_cat.shape[0]])\n",
    "print(data_cat.shape)\n",
    "print(dummy_labels.shape)\n",
    "data_train, data_test, _, __, = train_test_split(data_cat, dummy_labels, test_size = 0.1)\n",
    "\n",
    "encoding_dim = 30\n",
    "input_ = Input(shape=(data_cat.shape[1], ))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_)\n",
    "decoding_layer = Dense(data_cat.shape[1], activation = 'sigmoid')\n",
    "decoded = decoding_layer(encoded)\n",
    "\n",
    "autoencoder = Model(input_, decoded)\n",
    "encoder = Model(input_, encoded)\n",
    "\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "autoencoder.fit(data_train, data_train, epochs=50, validation_data=(data_test, data_test), verbose=False)\n",
    "\n",
    "# print(data_test.iloc[1].values.reshape(data_test.shape[1],1).shape)\n",
    "# print(data_test)\n",
    "# decoder.predict(encoder.predict(data_test)).round(0)\n",
    "# print(decoder.predict(encoder.predict(data_test.iloc[1].values.reshape(data_test.shape[1]))))\n",
    "\n",
    "# print(autoencoder.layers[-1].get_weights()[0])\n",
    "##############################################################\n",
    "num_points = encoding_dim\n",
    "print(num_points)\n",
    "gen_input_size = 10\n",
    "optimizer = Adam(0.0002, 0.9)\n",
    "batch_size = data_cat.shape[0]\n",
    "epochs = 5000\n",
    "steps_per_epoch = 1\n",
    "def generator():\n",
    "    generator = Sequential()\n",
    "    \n",
    "    generator.add(Dense(50, input_dim=gen_input_size))\n",
    "    generator.add(BatchNormalization())\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    generator.add(Dense(encoding_dim))\n",
    "    \n",
    "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return generator\n",
    "\n",
    "def discriminator():\n",
    "    discriminator = Sequential()\n",
    "    \n",
    "    discriminator.add(Dense(50, input_dim=num_points))\n",
    "    discriminator.add(BatchNormalization())\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    discriminator.compile(loss = 'binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return discriminator\n",
    "\n",
    "discriminator = discriminator()\n",
    "generator = generator()\n",
    "\n",
    "discriminator.trainable = False\n",
    "\n",
    "gan_input = Input(shape=(gen_input_size,))\n",
    "generated_patient = generator(gan_input)\n",
    "gan_output = discriminator(generated_patient)\n",
    "\n",
    "gan = Model(gan_input, gan_output)\n",
    "\n",
    "gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for steps in range(steps_per_epoch):\n",
    "        noise = sample_z(batch_size, gen_input_size)\n",
    "        fake_x = generator.predict(noise)\n",
    "        real_x = encoder.predict(data_cat.sample(n=batch_size))\n",
    "        x = np.concatenate((real_x, fake_x))\n",
    "        disc_y = np.zeros(2*batch_size)\n",
    "        disc_y[:batch_size] = 0.9\n",
    "        d_loss = discriminator.train_on_batch(x, disc_y)\n",
    "        y_gen = np.ones(batch_size)\n",
    "        g_loss = gan.train_on_batch(noise, y_gen)\n",
    "        \n",
    "    if epoch%1000 == 0:\n",
    "        print((time.time()-start)/60)\n",
    "        print(\"Iterations: %d\\t Discriminator loss: %.4f\\t Generator loss: %.4f\"%(epoch,d_loss,g_loss))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ['cp']_0  ['cp']_1  ['cp']_2  ['cp']_3  ['slope']_0  ['slope']_1  \\\n",
      "0        1.0       0.0       0.0       0.0          0.0          1.0   \n",
      "1        1.0       1.0       0.0       0.0          1.0          1.0   \n",
      "2        1.0       1.0       0.0       0.0          1.0          1.0   \n",
      "3        0.0       0.0       1.0       1.0          1.0          0.0   \n",
      "4        1.0       1.0       0.0       0.0          1.0          1.0   \n",
      "5        1.0       1.0       0.0       0.0          1.0          1.0   \n",
      "6        1.0       1.0       0.0       0.0          0.0          1.0   \n",
      "7        0.0       0.0       1.0       1.0          1.0          1.0   \n",
      "8        0.0       1.0       1.0       1.0          1.0          0.0   \n",
      "9        0.0       0.0       1.0       1.0          1.0          1.0   \n",
      "10       1.0       0.0       0.0       0.0          1.0          1.0   \n",
      "\n",
      "    ['slope']_2  ['thal']_0  ['thal']_1  ['thal']_2  ['thal']_3  ['target']_0  \\\n",
      "0           0.0         0.0         0.0         1.0         0.0           1.0   \n",
      "1           0.0         0.0         0.0         1.0         0.0           1.0   \n",
      "2           0.0         0.0         0.0         1.0         0.0           1.0   \n",
      "3           0.0         1.0         1.0         0.0         1.0           0.0   \n",
      "4           0.0         0.0         0.0         1.0         0.0           1.0   \n",
      "5           0.0         0.0         0.0         0.0         0.0           1.0   \n",
      "6           0.0         0.0         0.0         1.0         0.0           1.0   \n",
      "7           0.0         1.0         1.0         0.0         1.0           0.0   \n",
      "8           0.0         1.0         1.0         0.0         1.0           0.0   \n",
      "9           0.0         1.0         1.0         0.0         1.0           0.0   \n",
      "10          0.0         0.0         0.0         0.0         0.0           1.0   \n",
      "\n",
      "    ['target']_1  \n",
      "0            0.0  \n",
      "1            0.0  \n",
      "2            0.0  \n",
      "3            1.0  \n",
      "4            0.0  \n",
      "5            0.0  \n",
      "6            1.0  \n",
      "7            1.0  \n",
      "8            1.0  \n",
      "9            1.0  \n",
      "10           0.0  \n"
     ]
    }
   ],
   "source": [
    "a = generator.predict(sample_z(1, gen_input_size))\n",
    "cohort_size = 10\n",
    "for i in range(cohort_size):\n",
    "    a = np.concatenate((a, generator.predict(sample_z(1, gen_input_size))), axis=0)\n",
    "# print(data_cat.shape[1])\n",
    "data = decoder.predict(a).round(0)\n",
    "data_df = pd.DataFrame(data=data, columns = data_cat.columns.values)\n",
    "print(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07364900509516398\n",
      "Iterations: 0\t Discriminator loss: 1.0702\t Generator loss: 0.7048\n",
      "0.710802686214447\n",
      "Iterations: 1000\t Discriminator loss: 0.3579\t Generator loss: 2.9466\n",
      "1.3496827999750773\n",
      "Iterations: 2000\t Discriminator loss: 0.3403\t Generator loss: 2.2525\n",
      "1.9891611377398173\n",
      "Iterations: 3000\t Discriminator loss: 0.2877\t Generator loss: 2.5429\n",
      "2.6289148847262065\n",
      "Iterations: 4000\t Discriminator loss: 0.3638\t Generator loss: 2.6033\n",
      "3.268787682056427\n",
      "Iterations: 5000\t Discriminator loss: 1.0531\t Generator loss: 2.4568\n",
      "3.9091056187947593\n",
      "Iterations: 6000\t Discriminator loss: 0.8747\t Generator loss: 0.9968\n",
      "4.563680469989777\n",
      "Iterations: 7000\t Discriminator loss: 0.3759\t Generator loss: 2.0373\n",
      "5.207132951418559\n",
      "Iterations: 8000\t Discriminator loss: 0.2806\t Generator loss: 2.8732\n",
      "5.849877270062764\n",
      "Iterations: 9000\t Discriminator loss: 0.3487\t Generator loss: 1.9284\n",
      "6.50902335246404\n",
      "Iterations: 10000\t Discriminator loss: 0.4468\t Generator loss: 1.8486\n",
      "7.1528817017873125\n",
      "Iterations: 11000\t Discriminator loss: 0.3990\t Generator loss: 2.0635\n",
      "7.797216419378916\n",
      "Iterations: 12000\t Discriminator loss: 0.3261\t Generator loss: 2.7228\n",
      "8.441002082824706\n",
      "Iterations: 13000\t Discriminator loss: 0.1909\t Generator loss: 3.4284\n",
      "9.1194140513738\n",
      "Iterations: 14000\t Discriminator loss: 0.3706\t Generator loss: 1.6511\n",
      "9.772337218125662\n",
      "Iterations: 15000\t Discriminator loss: 0.3947\t Generator loss: 1.2029\n",
      "10.419859365622203\n",
      "Iterations: 16000\t Discriminator loss: 0.3600\t Generator loss: 2.0692\n",
      "11.06068293650945\n",
      "Iterations: 17000\t Discriminator loss: 0.5395\t Generator loss: 2.8460\n",
      "11.705550932884217\n",
      "Iterations: 18000\t Discriminator loss: 0.2980\t Generator loss: 2.1074\n",
      "12.35326810280482\n",
      "Iterations: 19000\t Discriminator loss: 0.4178\t Generator loss: 1.5959\n",
      "13.011628687381744\n",
      "Iterations: 20000\t Discriminator loss: 0.6124\t Generator loss: 3.1686\n",
      "13.65462883313497\n",
      "Iterations: 21000\t Discriminator loss: 0.3099\t Generator loss: 2.8253\n",
      "14.304896501700084\n",
      "Iterations: 22000\t Discriminator loss: 0.3185\t Generator loss: 2.2080\n",
      "14.951633818944295\n",
      "Iterations: 23000\t Discriminator loss: 0.3269\t Generator loss: 2.0620\n",
      "15.592554787794748\n",
      "Iterations: 24000\t Discriminator loss: 0.2120\t Generator loss: 4.7404\n",
      "16.231361031532288\n",
      "Iterations: 25000\t Discriminator loss: 0.2881\t Generator loss: 2.4071\n",
      "16.870867184797923\n",
      "Iterations: 26000\t Discriminator loss: 0.4227\t Generator loss: 2.0083\n",
      "17.515040866533916\n",
      "Iterations: 27000\t Discriminator loss: 0.5034\t Generator loss: 1.7666\n",
      "18.15403453509013\n",
      "Iterations: 28000\t Discriminator loss: 0.5205\t Generator loss: 2.4863\n",
      "18.796862185001373\n",
      "Iterations: 29000\t Discriminator loss: 0.4332\t Generator loss: 2.3182\n",
      "19.43720423380534\n",
      "Iterations: 30000\t Discriminator loss: 0.3208\t Generator loss: 1.8380\n",
      "20.077979453404744\n",
      "Iterations: 31000\t Discriminator loss: 0.2503\t Generator loss: 2.6534\n",
      "20.719825100898742\n",
      "Iterations: 32000\t Discriminator loss: 0.3117\t Generator loss: 2.0845\n",
      "21.358835717042286\n",
      "Iterations: 33000\t Discriminator loss: 0.3704\t Generator loss: 2.0058\n",
      "22.055008471012115\n",
      "Iterations: 34000\t Discriminator loss: 0.2768\t Generator loss: 2.4101\n",
      "22.747820083300272\n",
      "Iterations: 35000\t Discriminator loss: 0.6554\t Generator loss: 2.5886\n",
      "23.421154352029166\n",
      "Iterations: 36000\t Discriminator loss: 0.3653\t Generator loss: 1.6693\n",
      "24.083066900571186\n",
      "Iterations: 37000\t Discriminator loss: 0.4612\t Generator loss: 2.0037\n",
      "24.725662950674693\n",
      "Iterations: 38000\t Discriminator loss: 0.3376\t Generator loss: 1.9405\n",
      "25.374271134535473\n",
      "Iterations: 39000\t Discriminator loss: 0.3022\t Generator loss: 2.1398\n",
      "26.017313583691916\n",
      "Iterations: 40000\t Discriminator loss: 0.4073\t Generator loss: 2.8431\n",
      "26.659748351573946\n",
      "Iterations: 41000\t Discriminator loss: 0.3605\t Generator loss: 2.4362\n",
      "27.302017800013225\n",
      "Iterations: 42000\t Discriminator loss: 0.3782\t Generator loss: 1.7614\n",
      "27.944067533810934\n",
      "Iterations: 43000\t Discriminator loss: 0.4399\t Generator loss: 1.7995\n",
      "28.584266169865927\n",
      "Iterations: 44000\t Discriminator loss: 0.4878\t Generator loss: 2.1415\n",
      "29.23018986781438\n",
      "Iterations: 45000\t Discriminator loss: 0.5282\t Generator loss: 1.2607\n",
      "29.878043417135874\n",
      "Iterations: 46000\t Discriminator loss: 0.4967\t Generator loss: 1.8961\n",
      "30.52568426926931\n",
      "Iterations: 47000\t Discriminator loss: 0.3286\t Generator loss: 2.1783\n",
      "31.235307451089223\n",
      "Iterations: 48000\t Discriminator loss: 0.5277\t Generator loss: 1.4052\n",
      "31.879460187753043\n",
      "Iterations: 49000\t Discriminator loss: 0.2994\t Generator loss: 2.3721\n",
      "32.54600040117899\n",
      "Iterations: 50000\t Discriminator loss: 0.6868\t Generator loss: 1.8595\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('heart-disease-uci.zip')\n",
    "data_process = False\n",
    "if data_process:    \n",
    "    cont_ls = ['age', 'trestbps', 'chol', 'thalach','oldpeak']\n",
    "    data = minMaxNormalise(data, cont_ls)\n",
    "    cat_col = ['cp', 'slope', 'thal']\n",
    "    data = oneHotEnc(data, cat_col)\n",
    "\n",
    "target_df = data['target']\n",
    "variables = data.drop(['target'], axis=1)\n",
    "\n",
    "data_train, data_test, label_train, label_test = train_test_split(variables, target_df, test_size=0.20)\n",
    "gan_data = pd.concat([data_train, label_train], axis=1, sort=False)\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "tf.reset_default_graph()\n",
    "tf.keras.backend.clear_session\n",
    "start = time.time()\n",
    "num_points = gan_data.shape[1]\n",
    "print(num_points)\n",
    "gen_input_size = 100\n",
    "optimizer = Adam(0.0002, 0.9)\n",
    "batch_size = data_train.shape[0]\n",
    "epochs = 50001\n",
    "steps_per_epoch = 1\n",
    "def generator():\n",
    "    generator = Sequential()\n",
    "    \n",
    "    generator.add(Dense(500, input_dim=gen_input_size))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    generator.add(Dense(200))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    generator.add(Dense(50))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    generator.add(Dense(num_points))\n",
    "    \n",
    "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return generator\n",
    "\n",
    "def discriminator():\n",
    "    discriminator = Sequential()\n",
    "    \n",
    "    discriminator.add(Dense(200, input_dim=num_points))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    discriminator.add(Dense(50, input_dim=num_points))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    \n",
    "#     discriminator.add(Dense(50, input_dim=num_points))\n",
    "#     discriminator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    discriminator.compile(loss = 'binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return discriminator\n",
    "\n",
    "discriminator = discriminator()\n",
    "generator = generator()\n",
    "\n",
    "discriminator.trainable = False\n",
    "\n",
    "gan_input = Input(shape=(gen_input_size,))\n",
    "generated_patient = generator(gan_input)\n",
    "gan_output = discriminator(generated_patient)\n",
    "\n",
    "gan = Model(gan_input, gan_output)\n",
    "\n",
    "gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for steps in range(steps_per_epoch):\n",
    "        noise = sample_z(batch_size, gen_input_size)\n",
    "        fake_x = generator.predict(noise)\n",
    "        real_x = gan_data.sample(n=batch_size)\n",
    "        x = np.concatenate((real_x, fake_x))\n",
    "        disc_y = np.zeros(2*batch_size)\n",
    "        disc_y[:batch_size] = 0.9\n",
    "        d_loss = discriminator.train_on_batch(x, disc_y)\n",
    "        y_gen = np.ones(batch_size)\n",
    "        g_loss = gan.train_on_batch(noise, y_gen)\n",
    "        \n",
    "    if epoch%1000 == 0:\n",
    "        print((time.time()-start)/60)\n",
    "        print(\"Iterations: %d\\t Discriminator loss: %.4f\\t Generator loss: %.4f\"%(epoch,d_loss,g_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 14)\n",
      "count    242.000000\n",
      "mean       0.528926\n",
      "std        0.500197\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        1.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "Name: target, dtype: float64\n",
      "0\n",
      "Event rate in generated cohort: 0.00\n",
      "Event rate in test cohort: 0.61\n",
      "61\n",
      "37\n",
      "          age       sex        cp    trestbps        chol       fbs   restecg  \\\n",
      "0   99.684799  2.990803  1.109957  217.372803  339.374847  1.112384  0.166245   \n",
      "1  106.844696  3.159410  1.204707  232.597748  363.650482  1.212611  0.181576   \n",
      "2  116.297806  3.421916  1.315457  252.955475  395.680878  1.336409  0.201762   \n",
      "3  101.011963  3.015105  1.133956  220.234299  343.896088  1.127637  0.173851   \n",
      "4  102.971252  3.062208  1.160759  224.207687  350.319977  1.170142  0.177121   \n",
      "5  110.840881  3.243651  1.269091  240.856262  376.932373  1.275178  0.184759   \n",
      "6  109.073158  3.222993  1.226477  237.557419  371.303375  1.233913  0.186988   \n",
      "7  108.227135  3.157053  1.248366  235.001465  367.670837  1.255879  0.175291   \n",
      "8  110.379707  3.223912  1.264347  240.049988  375.719910  1.253366  0.185258   \n",
      "9  110.972725  3.242553  1.273971  241.287186  377.818451  1.259027  0.182036   \n",
      "\n",
      "      thalach     exang   oldpeak     slope        ca      thal    target  \n",
      "0  217.953049  4.211187 -0.261510  2.379620 -0.360258  2.982415  0.077338  \n",
      "1  232.969070  4.473681 -0.259989  2.489863 -0.370763  3.217036  0.069374  \n",
      "2  253.124496  4.852528 -0.272332  2.686090 -0.398614  3.506588  0.067683  \n",
      "3  220.813477  4.249836 -0.254556  2.392767 -0.358379  3.021451  0.069180  \n",
      "4  224.506332  4.324203 -0.266336  2.406922 -0.356832  3.092449  0.072618  \n",
      "5  240.986801  4.615690 -0.255254  2.537355 -0.362393  3.342765  0.056979  \n",
      "6  237.922409  4.563066 -0.263778  2.543898 -0.376568  3.280521  0.067876  \n",
      "7  234.867874  4.495790 -0.250245  2.442425 -0.347671  3.260172  0.051449  \n",
      "8  240.305542  4.592549 -0.255848  2.532110 -0.366182  3.336514  0.065710  \n",
      "9  241.593262  4.624881 -0.261334  2.548807 -0.368224  3.362910  0.075449  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-b2e1bfb3eb99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mlogreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1549\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1550\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    877\u001b[0m             raise ValueError(\"This solver needs samples of at least 2 classes\"\n\u001b[1;32m    878\u001b[0m                              \u001b[0;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                              \" class: %r\" % classes_[0])\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mclass_weight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0"
     ]
    }
   ],
   "source": [
    "a = generator.predict(sample_z(1, gen_input_size))\n",
    "cohort_size = 1000\n",
    "for i in range(cohort_size):\n",
    "    a = np.concatenate((a, generator.predict(sample_z(1, gen_input_size))), axis=0)\n",
    "print(a.shape)\n",
    "print(gan_data['target'].describe())\n",
    "b = pd.DataFrame(a, columns=gan_data.columns.values.tolist())\n",
    "target = b['target']\n",
    "variables = b.drop(['target'], axis=1)\n",
    "# print(target.head(10))\n",
    "target_ls = []\n",
    "target = target.round(0)\n",
    "for i in target:\n",
    "    if i>=1:\n",
    "        target_ls.append(1)\n",
    "    else:\n",
    "        target_ls.append(0)\n",
    "# print(target.describe())\n",
    "# print(gan_data.sample(n=1))\n",
    "total = 0\n",
    "for i in target_ls:\n",
    "    if i == 1:\n",
    "        total = total + 1\n",
    "        \n",
    "total_l = 0\n",
    "for i in label_test:\n",
    "    if i == 1:\n",
    "        total_l = total_l + 1\n",
    "print(total)\n",
    "\n",
    "print(\"Event rate in generated cohort: %.2f\" %(total/cohort_size))\n",
    "print(\"Event rate in test cohort: %.2f\" %(total_l/len(label_test)))\n",
    "print(len(label_test))\n",
    "print(total_l)\n",
    "print(b.head(10))\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(variables, target_ls)\n",
    "accuracy = round(logreg.score(data_test, label_test)*100, 2)\n",
    "print(accuracy)\n",
    "print(confusion_matrix(label_test, logreg.predict(data_test)))\n",
    "print(label_test.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
